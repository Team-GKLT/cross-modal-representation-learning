{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468bdc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a02b82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bb1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import random\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer, ViTModel, ViTFeatureExtractor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fb2abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>id</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48a9db19c2</td>\n",
       "      <td>f642f3cffc</td>\n",
       "      <td>8 stalks rhubarb, cut into 3 inch lengths 8 c...</td>\n",
       "      <td>387667</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1022, 29594, 1054, 6979, 8237, 2497, 101...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48a9c381f6</td>\n",
       "      <td>a8a884cafb</td>\n",
       "      <td>1 small head of garlic, peeled and sliced thi...</td>\n",
       "      <td>265527</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1015, 2235, 2132, 1997, 20548, 1010, 209...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48a94e34d4</td>\n",
       "      <td>09fde58055</td>\n",
       "      <td>2 cups flour zest from 1 lemon 1 tsp. Magic B...</td>\n",
       "      <td>15759</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 1016, 10268, 13724, 27838, 3367, 2013, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48a9634514</td>\n",
       "      <td>8392fc314a</td>\n",
       "      <td>9 ounces bittersweet chocolate, chopped 1 cup...</td>\n",
       "      <td>206796</td>\n",
       "      <td>3</td>\n",
       "      <td>[101, 1023, 19471, 2015, 8618, 26760, 15558, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48a3075256</td>\n",
       "      <td>b6feafd214</td>\n",
       "      <td>1 cup chickpeas, washed, picked over and soak...</td>\n",
       "      <td>288069</td>\n",
       "      <td>4</td>\n",
       "      <td>[101, 1015, 2452, 14556, 5051, 3022, 1010, 887...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       img_id     text_id                                        ingredients  \\\n",
       "0  48a9db19c2  f642f3cffc   8 stalks rhubarb, cut into 3 inch lengths 8 c...   \n",
       "1  48a9c381f6  a8a884cafb   1 small head of garlic, peeled and sliced thi...   \n",
       "2  48a94e34d4  09fde58055   2 cups flour zest from 1 lemon 1 tsp. Magic B...   \n",
       "3  48a9634514  8392fc314a   9 ounces bittersweet chocolate, chopped 1 cup...   \n",
       "4  48a3075256  b6feafd214   1 cup chickpeas, washed, picked over and soak...   \n",
       "\n",
       "       id  __index_level_0__  \\\n",
       "0  387667                  0   \n",
       "1  265527                  1   \n",
       "2   15759                  2   \n",
       "3  206796                  3   \n",
       "4  288069                  4   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1022, 29594, 1054, 6979, 8237, 2497, 101...   \n",
       "1  [101, 1015, 2235, 2132, 1997, 20548, 1010, 209...   \n",
       "2  [101, 1016, 10268, 13724, 27838, 3367, 2013, 1...   \n",
       "3  [101, 1023, 19471, 2015, 8618, 26760, 15558, 7...   \n",
       "4  [101, 1015, 2452, 14556, 5051, 3022, 1010, 887...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"/filer/tmp1/gg676/im2recipe/df_merged_appended_ingredients_tokenized.pkl\")[:8]\n",
    "#df_val = pd.read\n",
    "#df = df.sort_values(by='id')[:158915]\n",
    "#df.columns = ['text_id', 'img_id', 'ingredients']\n",
    "#df['ingredients'] = df['ingredients'].str.lstrip()\n",
    "#df['caption_number'] = df['caption_number'].str.lstrip()\n",
    "#df.loc[19999, 'caption_number'] = \"4\"\n",
    "#df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n",
    "#ids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\n",
    "#df['id'] = ids\n",
    "#df.to_csv(\"captions.csv\", index=False)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a952f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   img_id             8 non-null      object\n",
      " 1   text_id            8 non-null      object\n",
      " 2   ingredients        8 non-null      object\n",
      " 3   id                 8 non-null      int64 \n",
      " 4   __index_level_0__  8 non-null      int64 \n",
      " 5   input_ids          8 non-null      object\n",
      " 6   attention_mask     8 non-null      object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 576.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27caac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = \"/filer/tmp1/gg676/im2recipe/img_data/train_flattened\"\n",
    "    #captions_path = \".\"\n",
    "    batch_size = 4\n",
    "    num_workers = 4\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 3\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'vit_base_patch16_224'#'resnet50'\n",
    "    image_embedding = 1000\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edff74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e55a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, ingredients, ingredients_input_ids, ingredients_attn_mask, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.ingredients = [tokenizer(ingr, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for ingr in ingredients]\n",
    "        #self.ingredients = ingredients # list(ingredients)\n",
    "        #self.ingredients_input_ids = self.ingredients['input_ids']#ingredients_input_ids\n",
    "        #self.ingredients_attn_mask = self.ingredients['attention_mask']\n",
    "        #print(ingredients_input_ids.shape, \"--> \", ingredients_attn_mask.shape)\n",
    "        #self.encoded_ingredients = dict(zip(tuple(ingredients_input_ids[:]), ingredients_attn_mask))\n",
    "        #self.encoded_ingredients = tokenizer(\n",
    "        #    list(ingredients), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        #)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #item = {\n",
    "        #    key: torch.tensor(values)\n",
    "        #    for key, values in zip(self.ingredients_input_ids[idx], self.ingredients_attn_mask[idx])\n",
    "        #}\n",
    "        item = {}\n",
    "        #print(self.ingredients_input_ids[idx].shape)\n",
    "        #print(self.ingredients_input_ids[idx])\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}.jpg\")\n",
    "        #print(image)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        #print(image.shape)\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['ingredients'] = self.ingredients[idx]\n",
    "        #print(\"ingre: \", self.ingredients[idx], type(self.ingredients[idx]))\n",
    "        #return item#self.ingredients_input_ids[idx], torch.tensor(self.ingredients_attn_mask[idx])\n",
    "        return item['image'], self.ingredients[idx]['input_ids'], self.ingredients[idx]['attention_mask']\n",
    "        #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingredients)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9cf1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #self.model = timm.create_model(\n",
    "        #    model_name, pretrained=True, num_classes=0)#, num_classes=0, global_pool=\"avg\"\n",
    "        #)\n",
    "        self.model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0281f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained('/filer/tmp1/gg676/distilbert-base-uncased')\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state #last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b8e45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f1370e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_label_data(img_features, text_features):\n",
    "    concatenated_pairs = []\n",
    "    #print(\"shape of batch: \", img_features.shape, \"--> \", text_features.shape)\n",
    "    for each_img_idx in range(len(img_features)):\n",
    "        zero_label_samples = []\n",
    "        for each_img_img_idx in range(len(img_features)):\n",
    "            #print(\"count: \", each_img_img_idx)\n",
    "            if each_img_idx == each_img_img_idx:\n",
    "                correct_pairs = (torch.cat((img_features[each_img_idx], text_features[each_img_idx]), 0), 1)\n",
    "                concatenated_pairs.append(correct_pairs)\n",
    "                #concatenated_pairs.append((torch.cat(img_features[each_img_idx], text_features[each_img_idx]), 1)\n",
    "            else:\n",
    "                zero_label_samples.append((torch.cat((img_features[each_img_idx], text_features[each_img_img_idx]), 0), 0))\n",
    "        #sampled_zero_labels_tuples = [item for item in zero_label_samples if item[1] == 0]\n",
    "        #print(\"zero label: \", zero_label_samples[0])\n",
    "        concatenated_pairs.append(random.choice(zero_label_samples))\n",
    "    print(\"length: \", len(concatenated_pairs), \"--> \", len(concatenated_pairs[0][0]))\n",
    "    #print(\"\\n 2nd: \", concatenated_pairs[1], \"\\n\")\n",
    "    print(\"label: \", concatenated_pairs[0][1])\n",
    "    return list(zip(*concatenated_pairs))[0], list(zip(*concatenated_pairs))[1]\n",
    "    #for j in concatenated_pairs:\n",
    "    #    print(\"j -> \", j[1])\n",
    "    #return [i[0] for i in concatenated_pairs], [i[1] for i in concatenated_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaad8e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2a61b57a5a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCrossAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodal_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_image_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m197\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_positional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSinusoidalPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, modal_dim=768, n_heads=2, n_layers=2, num_image_patches=197, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.text_positional = SinusoidalPositionalEncoding(model_dim, dropout=dropout)\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "    \n",
    "    def forward(self, image_features, text_features, src_key_padding_mask=None):\n",
    "        batch_size = image_features.shape[0]\n",
    "        text_features = self.text_positional(text_features)\n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(image_features.shape[0], image_features.shape[1] + 1)))\n",
    "        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask=src_key_padding_mask)\n",
    "        projected_output = transformer_outputs[:, 0, :]\n",
    "        return self.cls_projection(projected_output)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7431d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c12be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        #self.image_encoder = self.image_encoder.to(CFG.device)\n",
    "        self.image_encoder.train()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        #self.text_encoder = DistilBertModel.from_pretrained('/filer/tmp1/gg676/distilbert-base-uncased')\n",
    "        #self.text_encoder = self.text_encoder.to(CFG.device)\n",
    "        self.text_encoder.train()\n",
    "        #classifier = classify(﻿128﻿,﻿100﻿,﻿17496﻿,﻿12﻿,﻿2﻿)\n",
    "        \n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.ntokens = 709  # size of vocabulary\n",
    "        self.emsize = 768  # embedding dimension\n",
    "        self.d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        self.nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        self.nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "        self.dropout = 0.2  # dropout probability\n",
    "        self.transformer_model = TransformerModel(self.ntokens, self.emsize, self.nhead, self.d_hid, self.nlayers, self.dropout).to(CFG.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, batch_img, input_ids, attn_mask):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch_img, output_hidden_states=True)\n",
    "        #print(image_features)\n",
    "        input_ids, attn_mask = input_ids.squeeze(1), attn_mask.squeeze(1)\n",
    "        print(\"image: \", image_features.hidden_states[-1].shape)\n",
    "        print(\"inp ids: \", input_ids.shape, \"--> \", attn_mask.shape)\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=input_ids, attention_mask=attn_mask\n",
    "        )\n",
    "        #print(self.text_encoder(input_ids=input_ids, attention_mask=attn_mask, output_hidden_states=True))\n",
    "        print(\"text: \", text_features.shape)\n",
    "        #print(\"image shape: \", image_features.shape)\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        #image_embeddings = self.image_projection(image_features)\n",
    "        #text_embeddings = self.text_projection(text_features)\n",
    "        concatenated_img_text_pairs, labels = create_binary_label_data(image_features.hidden_states[-1], text_features)\n",
    "        #loss, outputs = classifier.forward(concatenated_img_text_pairs, labels)\n",
    "        print(\"labels: \", labels)\n",
    "        print(concatenated_img_text_pairs[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        src_mask = generate_square_subsequent_mask(CFG.batch_size).to(CFG.device)\n",
    "        \n",
    "        output = self.transformer_model(concatenated_img_text_pairs, src_mask)\n",
    "        print(output.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        \"\"\"\n",
    "        #print(\"inside: \", loss.shape)\n",
    "        return loss.mean(dim=0)\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1079a77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "dim = 256\n",
    "embeddings = torch.randn(batch_size, dim)\n",
    "out = embeddings @ embeddings.T\n",
    "print(F.softmax(out, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0745a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = df#pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    #valid_ids = np.random.choice(\n",
    "    #    image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    #)\n",
    "    #train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe#[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    #valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe#, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"img_id\"].values,\n",
    "        dataframe[\"ingredients\"].values,\n",
    "        dataframe[\"input_ids\"].values,\n",
    "        dataframe[\"attention_mask\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fd138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57cf8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        #batch = {k: v.to('cuda:0') for k, v in batch.items() if k != \"ingredients\"}\n",
    "        img, input_ids, attn_mask = batch\n",
    "        #print(batch)\n",
    "        img = img.to(CFG.device)\n",
    "        #batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"ingredients\"}\n",
    "        input_ids = input_ids.to(CFG.device)\n",
    "        attn_mask = attn_mask.to(CFG.device)\n",
    "        loss = model(img, input_ids, attn_mask)\n",
    "        #print(\"outside: \", loss.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(torch.ones(1).to(CFG.device))\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"ingredients\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71812d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0,3'\n",
    "def main():\n",
    "    \n",
    "    train_df = make_train_valid_dfs()\n",
    "    start = time.time()\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('/filer/tmp1/gg676/distilbert-base-uncased')\n",
    "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "    print(\"Data loading time taken: \", time.time()-start)\n",
    "    model = CLIPModel()\n",
    "    params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.image_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "    #valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    print(\"Time taken to load: \", time.time()-start)\n",
    "    #if torch.cuda.device_count() > 1:\n",
    "    #  print(torch.cuda.device_count(), \"GPUs!\")\n",
    "    #  model = nn.DataParallel(model)\n",
    "    #model.to(f'cuda:{model.device_ids[0]}')\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        #model.eval()\n",
    "        #with torch.no_grad():\n",
    "        #    valid_loss = valid_epoch(model, valid_loader)\n",
    "        \n",
    "        if train_loss.avg < best_loss:\n",
    "            best_loss = train_loss.avg\n",
    "            torch.save(model.state_dict(), \"best_ilab2.pt\")\n",
    "            print(\"Saved Best Model!\")\n",
    "        \n",
    "        lr_scheduler.step(train_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc68d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading time taken:  0.03194570541381836\n",
      "Time taken to load:  88.91526508331299\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  torch.Size([4, 197, 768])\n",
      "inp ids:  torch.Size([4, 512]) -->  torch.Size([4, 512])\n",
      "text:  torch.Size([4, 512, 768])\n",
      "length:  8 -->  709\n",
      "label:  1\n",
      "labels:  (1, 0, 1, 0, 1, 0, 1, 0)\n",
      "tensor([[ 7.0166e-01,  7.4121e-01,  5.0344e+00,  ..., -2.7574e+00,\n",
      "         -4.8100e+00,  1.1782e+00],\n",
      "        [-1.4212e+00, -2.8038e+00,  6.1117e-01,  ..., -6.7750e+00,\n",
      "         -5.8535e+00,  3.1304e+00],\n",
      "        [ 7.8068e+00, -2.0451e+00, -4.4318e+00,  ..., -1.5728e+00,\n",
      "          6.7809e+00, -6.1711e+00],\n",
      "        ...,\n",
      "        [-4.9010e-02,  5.1514e-01,  1.5741e-01,  ...,  2.0217e-02,\n",
      "         -1.5868e-02, -2.8941e-01],\n",
      "        [-1.0092e-01,  3.4872e-01,  1.1845e-01,  ...,  1.2778e-02,\n",
      "         -1.0771e-02, -1.8734e-01],\n",
      "        [-1.9158e-02,  3.8705e-01,  1.2283e-02,  ...,  6.2120e-02,\n",
      "          5.6812e-03, -4.9909e-01]], device='cuda:3', grad_fn=<CatBackward0>)\n",
      "src:  (tensor([[ 7.0166e-01,  7.4121e-01,  5.0344e+00,  ..., -2.7574e+00,\n",
      "         -4.8100e+00,  1.1782e+00],\n",
      "        [-1.4212e+00, -2.8038e+00,  6.1117e-01,  ..., -6.7750e+00,\n",
      "         -5.8535e+00,  3.1304e+00],\n",
      "        [ 7.8068e+00, -2.0451e+00, -4.4318e+00,  ..., -1.5728e+00,\n",
      "          6.7809e+00, -6.1711e+00],\n",
      "        ...,\n",
      "        [-4.9010e-02,  5.1514e-01,  1.5741e-01,  ...,  2.0217e-02,\n",
      "         -1.5868e-02, -2.8941e-01],\n",
      "        [-1.0092e-01,  3.4872e-01,  1.1845e-01,  ...,  1.2778e-02,\n",
      "         -1.0771e-02, -1.8734e-01],\n",
      "        [-1.9158e-02,  3.8705e-01,  1.2283e-02,  ...,  6.2120e-02,\n",
      "          5.6812e-03, -4.9909e-01]], device='cuda:3', grad_fn=<CatBackward0>), tensor([[ 7.0166e-01,  7.4121e-01,  5.0344e+00,  ..., -2.7574e+00,\n",
      "         -4.8100e+00,  1.1782e+00],\n",
      "        [-1.4212e+00, -2.8038e+00,  6.1117e-01,  ..., -6.7750e+00,\n",
      "         -5.8535e+00,  3.1304e+00],\n",
      "        [ 7.8068e+00, -2.0451e+00, -4.4318e+00,  ..., -1.5728e+00,\n",
      "          6.7809e+00, -6.1711e+00],\n",
      "        ...,\n",
      "        [ 4.5171e-02,  3.1559e-01, -1.7501e-02,  ..., -4.4817e-03,\n",
      "         -8.0665e-02, -5.9101e-02],\n",
      "        [-1.1332e-02,  1.4231e-01, -3.0488e-02,  ...,  1.2860e-01,\n",
      "         -1.8016e-02, -2.4510e-01],\n",
      "        [-7.0348e-02, -7.1377e-02,  2.9279e-02,  ...,  9.6827e-02,\n",
      "         -1.9692e-01, -7.3458e-02]], device='cuda:3', grad_fn=<CatBackward0>), tensor([[-5.1312e+00,  4.5927e+00, -4.2506e-01,  ..., -4.5849e+00,\n",
      "         -4.5220e+00, -3.8777e+00],\n",
      "        [-1.3950e+00,  2.2443e+00, -3.3685e-01,  ..., -3.4056e+00,\n",
      "         -1.1193e+00, -3.8693e+00],\n",
      "        [-7.1982e+00, -9.9334e-01,  2.4896e+00,  ..., -2.7930e+00,\n",
      "         -2.9376e+00, -2.6193e+00],\n",
      "        ...,\n",
      "        [ 4.5171e-02,  3.1559e-01, -1.7501e-02,  ..., -4.4817e-03,\n",
      "         -8.0665e-02, -5.9101e-02],\n",
      "        [-1.1332e-02,  1.4231e-01, -3.0488e-02,  ...,  1.2860e-01,\n",
      "         -1.8016e-02, -2.4510e-01],\n",
      "        [-7.0348e-02, -7.1377e-02,  2.9279e-02,  ...,  9.6827e-02,\n",
      "         -1.9692e-01, -7.3458e-02]], device='cuda:3', grad_fn=<CatBackward0>), tensor([[-5.1312e+00,  4.5927e+00, -4.2506e-01,  ..., -4.5849e+00,\n",
      "         -4.5220e+00, -3.8777e+00],\n",
      "        [-1.3950e+00,  2.2443e+00, -3.3685e-01,  ..., -3.4056e+00,\n",
      "         -1.1193e+00, -3.8693e+00],\n",
      "        [-7.1982e+00, -9.9334e-01,  2.4896e+00,  ..., -2.7930e+00,\n",
      "         -2.9376e+00, -2.6193e+00],\n",
      "        ...,\n",
      "        [ 1.0589e-01,  1.0834e-01,  1.4400e-01,  ...,  3.4213e-02,\n",
      "         -1.7164e-01, -8.9096e-02],\n",
      "        [ 1.0212e-01,  7.9161e-02,  5.9390e-02,  ...,  1.3169e-01,\n",
      "         -3.1884e-01, -2.0998e-01],\n",
      "        [ 1.0668e-02,  9.7072e-02,  2.1891e-01,  ...,  3.5508e-03,\n",
      "         -2.5307e-01, -2.1155e-01]], device='cuda:3', grad_fn=<CatBackward0>), tensor([[ 1.6692e+00, -3.3790e+00,  1.5246e+00,  ...,  6.4654e+00,\n",
      "          4.5534e+00,  3.1859e-01],\n",
      "        [ 4.9052e+00,  3.0916e+00,  2.8632e+00,  ...,  1.4004e+01,\n",
      "          6.9181e+00, -2.3148e+00],\n",
      "        [ 2.9352e-01, -4.8864e+00, -1.9022e+00,  ...,  9.2918e+00,\n",
      "          9.5355e+00,  1.5341e+00],\n",
      "        ...,\n",
      "        [ 1.0589e-01,  1.0834e-01,  1.4400e-01,  ...,  3.4213e-02,\n",
      "         -1.7164e-01, -8.9096e-02],\n",
      "        [ 1.0212e-01,  7.9161e-02,  5.9390e-02,  ...,  1.3169e-01,\n",
      "         -3.1884e-01, -2.0998e-01],\n",
      "        [ 1.0668e-02,  9.7072e-02,  2.1891e-01,  ...,  3.5508e-03,\n",
      "         -2.5307e-01, -2.1155e-01]], device='cuda:3', grad_fn=<CatBackward0>), tensor([[ 1.6692e+00, -3.3790e+00,  1.5246e+00,  ...,  6.4654e+00,\n",
      "          4.5534e+00,  3.1859e-01],\n",
      "        [ 4.9052e+00,  3.0916e+00,  2.8632e+00,  ...,  1.4004e+01,\n",
      "          6.9181e+00, -2.3148e+00],\n",
      "        [ 2.9352e-01, -4.8864e+00, -1.9022e+00,  ...,  9.2918e+00,\n",
      "          9.5355e+00,  1.5341e+00],\n",
      "        ...,\n",
      "        [ 8.0341e-02,  3.7102e-01,  7.7952e-02,  ..., -8.3216e-02,\n",
      "         -2.1460e-02, -2.8613e-01],\n",
      "        [-6.0239e-03,  3.8003e-01,  3.3024e-01,  ...,  1.9829e-01,\n",
      "         -4.5472e-02, -5.1058e-01],\n",
      "        [ 4.0405e-02,  2.8886e-01,  1.1258e-01,  ...,  1.5637e-01,\n",
      "         -9.1225e-04, -4.2686e-01]], device='cuda:3', grad_fn=<CatBackward0>), tensor([[ 2.2295e+00, -2.0515e+00,  7.5707e+00,  ...,  1.4227e+00,\n",
      "         -1.1149e+00,  2.5773e-01],\n",
      "        [ 4.9388e+00,  4.2422e-01,  4.8602e+00,  ...,  2.0888e+00,\n",
      "         -1.0396e+00,  1.1185e+01],\n",
      "        [ 1.2016e+01,  2.0953e+00,  5.5446e+00,  ..., -8.0638e-01,\n",
      "         -3.6846e+00,  7.1664e+00],\n",
      "        ...,\n",
      "        [ 8.0341e-02,  3.7102e-01,  7.7952e-02,  ..., -8.3216e-02,\n",
      "         -2.1460e-02, -2.8613e-01],\n",
      "        [-6.0239e-03,  3.8003e-01,  3.3024e-01,  ...,  1.9829e-01,\n",
      "         -4.5472e-02, -5.1058e-01],\n",
      "        [ 4.0405e-02,  2.8886e-01,  1.1258e-01,  ...,  1.5637e-01,\n",
      "         -9.1225e-04, -4.2686e-01]], device='cuda:3', grad_fn=<CatBackward0>), tensor([[ 2.2295e+00, -2.0515e+00,  7.5707e+00,  ...,  1.4227e+00,\n",
      "         -1.1149e+00,  2.5773e-01],\n",
      "        [ 4.9388e+00,  4.2422e-01,  4.8602e+00,  ...,  2.0888e+00,\n",
      "         -1.0396e+00,  1.1185e+01],\n",
      "        [ 1.2016e+01,  2.0953e+00,  5.5446e+00,  ..., -8.0638e-01,\n",
      "         -3.6846e+00,  7.1664e+00],\n",
      "        ...,\n",
      "        [ 4.5171e-02,  3.1559e-01, -1.7501e-02,  ..., -4.4817e-03,\n",
      "         -8.0665e-02, -5.9101e-02],\n",
      "        [-1.1332e-02,  1.4231e-01, -3.0488e-02,  ...,  1.2860e-01,\n",
      "         -1.8016e-02, -2.4510e-01],\n",
      "        [-7.0348e-02, -7.1377e-02,  2.9279e-02,  ...,  9.6827e-02,\n",
      "         -1.9692e-01, -7.3458e-02]], device='cuda:3', grad_fn=<CatBackward0>))\n",
      "d_model:  torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-f20d61a3d79b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m#model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#with torch.no_grad():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-68bef5b8f0c1>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, step)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(\"outside: \", loss.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-681b35df6fb9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_img, input_ids, attn_mask)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_img_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-82bb802d72af>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"d_model: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bacc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb249f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42033a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(CFG.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "    \n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "    print(matches)\n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{match}.jpg\")\n",
    "        #print(image)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches(model, \n",
    "             image_embeddings,\n",
    "             query=\"Chicken thighs\",\n",
    "             image_filenames=valid_df['img_id'].values,\n",
    "             n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def578ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(img, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0479034",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = image_encoder(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[-1][0][1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655af767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from PIL import Image\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)\n",
    "\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "img = Image.open(filename).convert('RGB')\n",
    "tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737caf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcaf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce237bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
