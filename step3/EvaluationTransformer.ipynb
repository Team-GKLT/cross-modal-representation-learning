{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "6c157dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import random\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer, ViTModel, ViTFeatureExtractor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d3766080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  8 20:53:24 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000    On   | 00000000:1A:00.0 Off |                 Off* |\n",
      "| 41%   40C    P8    15W / 140W |  16109MiB / 16117MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000    On   | 00000000:1B:00.0 Off |                 Off* |\n",
      "| 41%   36C    P8    14W / 140W |   6230MiB / 16117MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000    On   | 00000000:60:00.0 Off |                 Off* |\n",
      "| 41%   40C    P8    14W / 140W |  12537MiB / 16117MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000    On   | 00000000:61:00.0 Off |                 Off* |\n",
      "| 41%   45C    P8    16W / 140W |  11691MiB / 16117MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    417364      C   /opt/conda/bin/python3.8        12151MiB |\n",
      "|    0   N/A  N/A   1795094      C   /opt/conda/bin/python3.8         3955MiB |\n",
      "|    1   N/A  N/A   1795094      C   /opt/conda/bin/python3.8         6225MiB |\n",
      "|    2   N/A  N/A    263539      C   python                          10913MiB |\n",
      "|    2   N/A  N/A   3654870      C   /opt/conda/bin/python3.8         1621MiB |\n",
      "|    3   N/A  N/A    263539      C   python                           4387MiB |\n",
      "|    3   N/A  N/A   3912467      C   /opt/conda/bin/python3.8         7301MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f60921e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = '/common/home/gg676/536/notebooks/best_ilab2_scaled_new.pt'#best_ilab2.pt'#_epoch2_concatenated_ingr_620k.pt'\n",
    "TEST_DATA = '/filer/tmp1/gg676/im2recipe/df_merged_TEST_appended_ingredients.pkl'#df_merged_TEST_appended_ingredients.pkl'#df_merged_VAL_appended_ingredients.pkl'#df_merged_VAL_ingredients.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3acf5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_pickle(TEST_DATA)[:10]\n",
    "df_val.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "aa1d56bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   img_id       10 non-null     object\n",
      " 1   text_id      10 non-null     object\n",
      " 2   ingredients  10 non-null     object\n",
      " 3   id           10 non-null     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 448.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d72f5665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ff611e83ca</td>\n",
       "      <td>3a5be7f0f5</td>\n",
       "      <td>100 grams Sweet potato 3 tbsp Honey 3 tbsp Mi...</td>\n",
       "      <td>91864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7097aa2903</td>\n",
       "      <td>54755e3dd8</td>\n",
       "      <td>750 ml vodka 3 -4 jalapenos 3 -4 red chilies ...</td>\n",
       "      <td>133073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78e994f1d6</td>\n",
       "      <td>3e47ed2ffa</td>\n",
       "      <td>3 ounces dark rum (or to suit taste) 4 tables...</td>\n",
       "      <td>97999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3b1f774356</td>\n",
       "      <td>a82c3d3651</td>\n",
       "      <td>1 (18 1/4 ounce) package fudge cake mix 3 tab...</td>\n",
       "      <td>264757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3eaa3482d4</td>\n",
       "      <td>fd9fc0c152</td>\n",
       "      <td>1 orange Safeway 1 lb For $1.28 thru 02/09 bu...</td>\n",
       "      <td>399129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       img_id     text_id                                        ingredients  \\\n",
       "0  ff611e83ca  3a5be7f0f5   100 grams Sweet potato 3 tbsp Honey 3 tbsp Mi...   \n",
       "1  7097aa2903  54755e3dd8   750 ml vodka 3 -4 jalapenos 3 -4 red chilies ...   \n",
       "2  78e994f1d6  3e47ed2ffa   3 ounces dark rum (or to suit taste) 4 tables...   \n",
       "3  3b1f774356  a82c3d3651   1 (18 1/4 ounce) package fudge cake mix 3 tab...   \n",
       "4  3eaa3482d4  fd9fc0c152   1 orange Safeway 1 lb For $1.28 thru 02/09 bu...   \n",
       "\n",
       "       id  \n",
       "0   91864  \n",
       "1  133073  \n",
       "2   97999  \n",
       "3  264757  \n",
       "4  399129  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "34072fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = \"/filer/tmp1/gg676/im2recipe/img_data/train_flattened\"\n",
    "    val_path = \"/common/users/gg676/test_flattened\"#test_flattened\"\n",
    "    #captions_path = \".\"\n",
    "    batch_size = 48\n",
    "    num_workers = 16\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    cross_attn_lr = 1e-4\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 3\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'vit_base_patch16_224'#'resnet50'\n",
    "    image_embedding = 1000\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "8638790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, ingredients, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        #self.tokenizer_need = tokenizer_need\n",
    "        self.ingredients = [tokenizer(ingr, \n",
    "                               padding='max_length', max_length = 128, truncation=True,\n",
    "                                return_tensors=\"pt\") for ingr in ingredients]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        image = cv2.imread(f\"{CFG.val_path}/{self.image_filenames[idx]}.jpg\")\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['ingredients'] = self.ingredients[idx]\n",
    "        return item['image'], self.ingredients[idx]['input_ids'], self.ingredients[idx]['attention_mask']\n",
    "        #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingredients)\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0874be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDatasetPreprocessed(torch.utils.data.Dataset):\n",
    "    def __init__(self, image, input_ids, attn_mask, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image = image\n",
    "        #self.tokenizer_need = tokenizer_need\n",
    "        #self.ingredients = [tokenizer(ingr, \n",
    "        #                       padding='max_length', max_length = 128, truncation=True,\n",
    "        #                        return_tensors=\"pt\") for ingr in ingredients]\n",
    "        self.input_ids = input_ids\n",
    "        self.attn_mask = attn_mask\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #item = {}\n",
    "        #image = cv2.imread(f\"{CFG.val_path}/{self.image_filenames[idx]}.jpg\")\n",
    "        #image = self.transforms(image=image)['image']\n",
    "        #item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        #item['ingredients'] = self.ingredients[idx]\n",
    "        return self.image[idx], self.input_ids[idx], self.attn_mask[idx]\n",
    "        #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c6d36406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs(df, mode='train'):\n",
    "    dataframe = df#pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    if mode == 'train':\n",
    "        train_dataframe = dataframe#[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "        return train_dataframe\n",
    "    else:\n",
    "        valid_dataframe = dataframe#[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "        return valid_dataframe\n",
    "    #return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"img_id\"].values,\n",
    "        dataframe[\"ingredients\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def build_preprocessed_loaders(img, input_ids, attn_mask, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDatasetPreprocessed(\n",
    "        img,\n",
    "        input_ids,\n",
    "        attn_mask,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "91b149e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "22dee225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained('/filer/tmp1/gg676/distilbert-base-uncased')\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state #last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e6855015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0a5a11f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CrossAttention(nn.Module):\\n    def __init__(self, model_dim=768, n_heads=8, n_layers=4, num_image_patches=197, num_classes=1, dropout=0.3):\\n        super().__init__()\\n        self.text_positional = PositionalEncoding(model_dim, dropout=dropout)\\n        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\\n        layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\\n        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\\n        self.cls_projection = nn.Linear(model_dim, num_classes)\\n        self.sigmoid = nn.Sigmoid()\\n    \\n    def forward(self, image_features, text_features, src_key_padding_mask=None):\\n        #print(image_features.shape)\\n        batch_size = image_features.shape[0]\\n        text_features = self.text_positional(text_features)\\n        sep_token = self.sep_token.expand(batch_size, -1, -1)\\n        transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\\n        if src_key_padding_mask is not None:\\n            src_key_padding_mask = torch.cat((torch.zeros(image_features.shape[0], \\n                                                          image_features.shape[1] + 1).to(CFG.device),\\n                                             src_key_padding_mask.to(CFG.device)), 1)\\n            \\n        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask=src_key_padding_mask)\\n        projected_output = transformer_outputs[:, 0, :]\\n        return self.sigmoid(self.cls_projection(projected_output))\\n        \\n'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, n_heads=2, n_layers=2, num_image_patches=197, num_classes=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.text_positional = PositionalEncoding(model_dim, dropout=dropout)\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, image_features, text_features, src_key_padding_mask=None):\n",
    "        #print(image_features.shape)\n",
    "        batch_size = image_features.shape[0]\n",
    "        text_features = self.text_positional(text_features)\n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(image_features.shape[0], \n",
    "                                                          image_features.shape[1] + 1).to(CFG.device),\n",
    "                                             src_key_padding_mask.to(CFG.device)), 1)\n",
    "            \n",
    "        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask=src_key_padding_mask)\n",
    "        projected_output = transformer_outputs[:, 0, :]\n",
    "        return self.sigmoid(self.cls_projection(projected_output))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, n_heads=8, n_layers=4, num_image_patches=197, num_classes=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.text_positional = PositionalEncoding(model_dim, dropout=dropout)\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, image_features, text_features, src_key_padding_mask=None):\n",
    "        #print(image_features.shape)\n",
    "        batch_size = image_features.shape[0]\n",
    "        text_features = self.text_positional(text_features)\n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(image_features.shape[0], \n",
    "                                                          image_features.shape[1] + 1).to(CFG.device),\n",
    "                                             src_key_padding_mask.to(CFG.device)), 1)\n",
    "            \n",
    "        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask=src_key_padding_mask)\n",
    "        projected_output = transformer_outputs[:, 0, :]\n",
    "        return self.sigmoid(self.cls_projection(projected_output))\n",
    "        \n",
    "\"\"\"       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4d86840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, n_heads=8, n_layers=4, num_image_patches=197, num_classes=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        #self.text_positional = PositionalEncoding(model_dim, dropout=dropout)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim, dropout)\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, image_features, text_features, src_key_padding_mask=None):\n",
    "        #print(image_features.shape)\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        \n",
    "        \n",
    "        batch_size = image_features.shape[0]\n",
    "        #image_features = self.text_positional(image_features)\n",
    "        #text_features = self.text_positional(text_features)\n",
    "        \n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        src = torch.cat((image_features, sep_token, text_features), dim=1) * math.sqrt(768)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        \n",
    "        #sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        #transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(image_features.shape[0], \n",
    "                                                          image_features.shape[1] + 1).to(CFG.device),\n",
    "                                             src_key_padding_mask.to(CFG.device)), 1)\n",
    "            \n",
    "        transformer_outputs = self.encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        projected_output = transformer_outputs[:, 0, :]\n",
    "        return self.sigmoid(self.cls_projection(projected_output))\n",
    "        \n",
    "\"\"\"\n",
    "        \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, n_heads=8, n_layers=4, num_image_patches=197, num_classes=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.text_positional = PositionalEncoding(model_dim, dropout=dropout)\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, image_features, text_features, src_key_padding_mask=None):\n",
    "        #print(image_features.shape)\n",
    "        batch_size = image_features.shape[0]\n",
    "        text_features = self.text_positional(text_features)\n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(image_features.shape[0], \n",
    "                                                          image_features.shape[1] + 1).to(CFG.device),\n",
    "                                             src_key_padding_mask.to(CFG.device)), 1)\n",
    "            \n",
    "        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask=src_key_padding_mask)\n",
    "        projected_output = transformer_outputs[:, 0, :]\n",
    "        return self.sigmoid(self.cls_projection(projected_output))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5f3cfebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        #self.image_encoder = self.image_encoder.to(CFG.device)\n",
    "        self.image_encoder.eval()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        #self.text_encoder = DistilBertModel.from_pretrained('/filer/tmp1/gg676/distilbert-base-uncased')\n",
    "        #self.text_encoder = self.text_encoder.to(CFG.device)\n",
    "        self.text_encoder.eval()\n",
    "        #classifier = classify(﻿128﻿,﻿100﻿,﻿17496﻿,﻿12﻿,﻿2﻿)\n",
    "        \n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.ntokens = 709  # size of vocabulary\n",
    "        self.emsize = 768  # embedding dimension\n",
    "        self.d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        self.nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        self.nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "        self.dropout = 0.2  # dropout probability\n",
    "        #self.transformer_model = TransformerModel(self.ntokens, self.emsize, self.nhead, self.d_hid, self.nlayers, self.dropout).to(CFG.device)\n",
    "        self.cross_attn = CrossAttention()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, batch_img, input_ids, attn_mask):\n",
    "        # Getting Image and Text Features\n",
    "        with torch.no_grad():\n",
    "            image_features = self.image_encoder(batch_img, output_hidden_states=True)\n",
    "            #print(image_features)\n",
    "            #print(input_ids.shape, attn_mask.shape)\n",
    "            input_ids, attn_mask = input_ids.squeeze(1), attn_mask.squeeze(1)\n",
    "            #print(\"image: \", image_features.hidden_states[-1].shape)\n",
    "            #print(\"inp ids: \", input_ids.shape, \"--> \", attn_mask.shape)\n",
    "            text_features = self.text_encoder(\n",
    "                input_ids=input_ids, attention_mask=attn_mask\n",
    "        )\n",
    "        #print(self.text_encoder(input_ids=input_ids, attention_mask=attn_mask, output_hidden_states=True))\n",
    "        #print(\"text: \", text_features.shape)\n",
    "        #print(\"image shape: \", image_features.shape)\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        #image_embeddings = self.image_projection(image_features)\n",
    "        #text_embeddings = self.text_projection(text_features)\n",
    "        #img_feature, text_feature, attn_mask_list, labels = create_binary_label_data(image_features.hidden_states[-1], text_features, attn_mask)\n",
    "        #loss, outputs = classifier.forward(concatenated_img_text_pairs, labels)\n",
    "        #print(\"labels: \", labels)\n",
    "        #print(concatenated_img_text_pairs[0])\n",
    "        #print(img_feature)\n",
    "        #print(img_feature[0].shape)\n",
    "        \n",
    "        \n",
    "        #src_mask = generate_square_subsequent_mask(CFG.batch_size).to(CFG.device)\n",
    "        \n",
    "        output = self.cross_attn(img_feature, text_feature, attn_mask)\n",
    "        #print(output.shape)\n",
    "        #print(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        \"\"\"\n",
    "        #print(\"inside: \", loss.shape)\n",
    "        return output, labels.unsqueeze(1)#.to(CFG.device)\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "5e5e9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "valid_loader = build_loaders(df_val, tokenizer, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4a9bca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_image_textvalid_df(valid_loader):\n",
    "    \n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    model.image_encoder.eval()\n",
    "    model.text_encoder.eval()\n",
    "    img_all_list = []\n",
    "    input_id_list = []\n",
    "    attn_mask_list = []\n",
    "    text_all_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            img, input_ids, attn_mask = batch\n",
    "            input_ids, attn_mask = input_ids.squeeze(1), attn_mask.squeeze(1)\n",
    "            image_features = model.image_encoder(img.to(CFG.device), output_hidden_states=True).hidden_states[-1]\n",
    "            text_features = model.text_encoder(input_ids=input_ids.to(CFG.device), attention_mask=attn_mask.to(CFG.device))\n",
    "            #print(attn_mask)\n",
    "            img_all_list.extend(image_features)\n",
    "            text_all_list.extend(text_features)\n",
    "            #input_id_list.extend(input_ids)\n",
    "            attn_mask_list.extend(attn_mask)\n",
    "    #print(len(attn_mask_list))\n",
    "    return img_all_list, text_all_list, attn_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "87a357e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "source": [
    "img_all_list, text_all_list, attn_mask_list = prep_image_textvalid_df(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "0df3d6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "59f9a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_comb_img_text(img_all_list, text_all_list, attn_mask_list):\n",
    "    img_final_list = []\n",
    "    text_final_list = []\n",
    "    attn_mask_final_list = []\n",
    "    count = 0\n",
    "    for i in img_all_list:\n",
    "        #print(i)\n",
    "        for text, attn in zip(text_all_list, attn_mask_list):\n",
    "            img_final_list.append(i)\n",
    "            text_final_list.append(text)\n",
    "            attn_mask_final_list.append(attn)\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "    #print(img_final_list[0])\n",
    "    return img_final_list, text_final_list, attn_mask_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "6215fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "img_final_list,text_final_list, attn_mask_list = create_all_comb_img_text(img_all_list, text_all_list, attn_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "eb56a7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "49328db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brute prediction so sad\n",
    "def get_pred_all_comb(img_final_list, input_id_final_list, attn_mask_list):\n",
    "    #valid_loader = build_preprocessed_loaders(img_final_list, text_final_list, attn_mask_list,  mode=\"valid\")\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    model.image_encoder.eval()\n",
    "    model.text_encoder.eval()\n",
    "    model.cross_attn.eval()\n",
    "    output_matrix = []\n",
    "    with torch.no_grad():\n",
    "        #for batch in tqdm(valid_loader):\n",
    "            #img_features, text_features, attn_mask = batch\n",
    "        for idx, img_features in enumerate(img_final_list):\n",
    "            output_for_each_img = []\n",
    "            for text_features, attn_mask in zip(input_id_final_list, attn_mask_list):\n",
    "                #input_ids, attn_mask = input_ids.squeeze(1), attn_mask.squeeze(1)\n",
    "                #image_features = model.image_encoder(img.to(CFG.device), output_hidden_states=True)\n",
    "                #print(\"image features shape: \", image_features.hidden_states[-1].shape)\n",
    "                #text_features = model.text_encoder(input_ids=input_ids.to(CFG.device), attention_mask=attn_mask.to(CFG.device))\n",
    "                output_for_each_img.extend(model.cross_attn(img_features.unsqueeze(0), text_features.unsqueeze(0), attn_mask.unsqueeze(0)).detach().cpu().numpy()[0])\n",
    "            print(\"idx: \", idx)\n",
    "            output_matrix.append(output_for_each_img)\n",
    "                \n",
    "    return output_matrix     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "211188d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  0\n",
      "idx:  1\n",
      "idx:  2\n",
      "idx:  3\n",
      "idx:  4\n",
      "idx:  5\n",
      "idx:  6\n",
      "idx:  7\n",
      "idx:  8\n",
      "idx:  9\n",
      "idx:  10\n",
      "idx:  11\n",
      "idx:  12\n",
      "idx:  13\n",
      "idx:  14\n",
      "idx:  15\n",
      "idx:  16\n",
      "idx:  17\n",
      "idx:  18\n",
      "idx:  19\n",
      "idx:  20\n",
      "idx:  21\n",
      "idx:  22\n",
      "idx:  23\n",
      "idx:  24\n",
      "idx:  25\n",
      "idx:  26\n",
      "idx:  27\n",
      "idx:  28\n",
      "idx:  29\n",
      "idx:  30\n",
      "idx:  31\n",
      "idx:  32\n",
      "idx:  33\n",
      "idx:  34\n",
      "idx:  35\n",
      "idx:  36\n",
      "idx:  37\n",
      "idx:  38\n",
      "idx:  39\n",
      "idx:  40\n",
      "idx:  41\n",
      "idx:  42\n",
      "idx:  43\n",
      "idx:  44\n",
      "idx:  45\n",
      "idx:  46\n",
      "idx:  47\n",
      "idx:  48\n",
      "idx:  49\n",
      "idx:  50\n",
      "idx:  51\n",
      "idx:  52\n",
      "idx:  53\n",
      "idx:  54\n",
      "idx:  55\n",
      "idx:  56\n",
      "idx:  57\n",
      "idx:  58\n",
      "idx:  59\n",
      "idx:  60\n",
      "idx:  61\n",
      "idx:  62\n",
      "idx:  63\n",
      "idx:  64\n",
      "idx:  65\n",
      "idx:  66\n",
      "idx:  67\n",
      "idx:  68\n",
      "idx:  69\n",
      "idx:  70\n",
      "idx:  71\n",
      "idx:  72\n",
      "idx:  73\n",
      "idx:  74\n",
      "idx:  75\n",
      "idx:  76\n",
      "idx:  77\n",
      "idx:  78\n",
      "idx:  79\n",
      "idx:  80\n",
      "idx:  81\n",
      "idx:  82\n",
      "idx:  83\n",
      "idx:  84\n",
      "idx:  85\n",
      "idx:  86\n",
      "idx:  87\n",
      "idx:  88\n",
      "idx:  89\n",
      "idx:  90\n",
      "idx:  91\n",
      "idx:  92\n",
      "idx:  93\n",
      "idx:  94\n",
      "idx:  95\n",
      "idx:  96\n",
      "idx:  97\n",
      "idx:  98\n",
      "idx:  99\n"
     ]
    }
   ],
   "source": [
    "output_matrix = get_pred_all_comb(img_final_list, text_final_list, attn_mask_list)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "65f2a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_text_features(valid_df):\n",
    "    #tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    valid_loader = build_preprocessed_loaders(img_final_list, input_id_final_list, attn_mask_list,  mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    model.image_encoder.eval()\n",
    "    model.text_encoder.eval()\n",
    "    model.cross_attn.eval()\n",
    "    output_list = []\n",
    "    #img_id_list = []\n",
    "    img_feature_list = []\n",
    "    text_feature_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            img, input_ids, attn_mask = batch\n",
    "            #img_id_list.extend(img_id)\n",
    "            input_ids, attn_mask = input_ids.squeeze(1), attn_mask.squeeze(1)\n",
    "            image_features = model.image_encoder(img.to(CFG.device), output_hidden_states=True)\n",
    "            #print(\"image features shape: \", image_features.hidden_states[-1].shape)\n",
    "            text_features = model.text_encoder(input_ids=input_ids.to(CFG.device), attention_mask=attn_mask.to(CFG.device))\n",
    "            output = model.cross_attn(image_features.hidden_states[-1], text_features, attn_mask)\n",
    "            output_list.extend(output.detach().cpu().numpy())\n",
    "            img_feature_list.extend(image_features.hidden_states[-1].detach().cpu().numpy())\n",
    "            text_feature_list.extend(text_features.detach().cpu().numpy())\n",
    "            #image_embeddings = model.image_projection(image_features)\n",
    "            #valid_image_featur.append(image_features)\n",
    "    return output_list, img_feature_list, text_feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_image_text_features(valid_df):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, tokenizer_need=False, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    model.image_encoder.eval()\n",
    "    model.text_encoder.eval()\n",
    "    model.cross_attn.eval()\n",
    "    output_list = []\n",
    "    img_feature_list = []\n",
    "    text_feature_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            img, input_ids, attn_mask = batch\n",
    "            input_ids, attn_mask = input_ids.squeeze(1), attn_mask.squeeze(1)\n",
    "            image_features = model.image_encoder(img.to(CFG.device), output_hidden_states=True)\n",
    "            #print(\"image features shape: \", image_features.hidden_states[-1].shape)\n",
    "            text_features = model.text_encoder(input_ids=input_ids.to(CFG.device), attention_mask=attn_mask.to(CFG.device))\n",
    "            output = model.cross_attn(image_features.hidden_states[-1], text_features, attn_mask)\n",
    "            output_list.extend(output.detach().cpu().numpy())\n",
    "            img_feature_list.extend(image_features.hidden_states[-1].detach().cpu().numpy())\n",
    "            text_feature_list.extend(text_features.detach().cpu().numpy())\n",
    "            #image_embeddings = model.image_projection(image_features)\n",
    "            #valid_image_featur.append(image_features)\n",
    "    return output_list, img_feature_list, text_feature_list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_df = make_train_valid_dfs(mode='valid')\n",
    "output_list, img_feature_list, text_feature_list = get_image_text_features(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a75deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_feature_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196a6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list_values = [value.item() for value in output_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f441d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73335478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5d251680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(output_matrix):\n",
    "    results_dict = {}\n",
    "    #projection_txt, projection_img = txt_data, img_data\n",
    "    med_dict = {}\n",
    "    idxs = range(100)\n",
    "    \n",
    "    glob_rank = []\n",
    "    glob_recall = {1:0.0,5:0.0,10:0.0}\n",
    "    \n",
    "    for i in range(10):\n",
    "        ids = random.sample(range(0,len(output_matrix)-1), 10)\n",
    "        \n",
    "        #txt_sample = projection_txt[ids,:]\n",
    "        #img_sample = projection_img[ids,:]\n",
    "        \n",
    "        similarity = np.array(output_matrix)#np.dot(txt_sample.cpu().numpy(), img_sample.T.cpu().numpy())\n",
    "\n",
    "        med_rank = []\n",
    "        \n",
    "        recall = {1:0.0,5:0.0,10:0.0}\n",
    "        #print(idxs)\n",
    "        for ii in range(10):\n",
    "            #print(ii)\n",
    "            # get a column of similarities\n",
    "            sim = similarity[ii, :]\n",
    "            # sort indices in descending order\n",
    "            sorting = np.argsort(sim)[::-1].tolist()\n",
    "            # find where the index of the pair sample ended up in the sorting\n",
    "            pos = sorting.index(ii)  \n",
    "            if (pos+1) == 1:\n",
    "                recall[1]+=1\n",
    "            if (pos+1) <=5:\n",
    "                recall[5]+=1\n",
    "            if (pos+1)<=10:\n",
    "                recall[10]+=1\n",
    "            # store the position\n",
    "            med_rank.append(pos+1)\n",
    "        for i in recall.keys():\n",
    "            recall[i]=recall[i]/10\n",
    "        med = np.median(med_rank)\n",
    "        for i in recall.keys():\n",
    "            glob_recall[i]+=recall[i]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] = glob_recall[i]/10\n",
    "    \n",
    "    med_dict[\"mean_median\"] = np.average(glob_rank)\n",
    "    med_dict[\"recall\"] = glob_recall\n",
    "    med_dict[\"median_all\"] = glob_rank\n",
    "    print(\"Result:\",med_dict)\n",
    "    return med_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "6988dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'mean_median': 30.5, 'recall': {1: 0.0, 5: 0.0, 10: 0.0}, 'median_all': [30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_median': 30.5,\n",
       " 'recall': {1: 0.0, 5: 0.0, 10: 0.0},\n",
       " 'median_all': [30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5, 30.5]}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(output_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fca4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
